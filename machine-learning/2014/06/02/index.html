<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>实现一个反向传播人工神经网络 | Reverland的行知阁</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="machine learning,python," />
  

  <meta name="description" content="为何实现一个BP神经网络？
“What I cannot create, I do not understand”
— Richard Feynman,  February 1988


实现一个BP神经网络的7个步骤
选择神经网络 结构
随机 初始化权重
实现 前向传播 
实现 成本函数 $J(\Theta)$
实现反向传播算法并计算 偏微分 $\frac{\partial}{\partial\">
<meta property="og:type" content="article">
<meta property="og:title" content="实现一个反向传播人工神经网络">
<meta property="og:url" content="http://reverland.org/machine-learning/2014/06/02//index.html">
<meta property="og:site_name" content="Reverland的行知阁">
<meta property="og:description" content="为何实现一个BP神经网络？
“What I cannot create, I do not understand”
— Richard Feynman,  February 1988


实现一个BP神经网络的7个步骤
选择神经网络 结构
随机 初始化权重
实现 前向传播 
实现 成本函数 $J(\Theta)$
实现反向传播算法并计算 偏微分 $\frac{\partial}{\partial\">
<meta property="og:image" content="http://reverland.org/images/nn.png">
<meta property="og:updated_time" content="2015-11-15T06:01:31.527Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="实现一个反向传播人工神经网络">
<meta name="twitter:description" content="为何实现一个BP神经网络？
“What I cannot create, I do not understand”
— Richard Feynman,  February 1988


实现一个BP神经网络的7个步骤
选择神经网络 结构
随机 初始化权重
实现 前向传播 
实现 成本函数 $J(\Theta)$
实现反向传播算法并计算 偏微分 $\frac{\partial}{\partial\">

  

  
    <link rel="icon" href="/images/favicon.ico">
  

  

  <link href="/css/styles.css?v=ed12202f" rel="stylesheet">

  

  

</head>

<body>

  
    <a href="#modal-one" class="toolbox-mobile">盒子</a>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a 
            class="CIRCLE" 
            href="/archives/"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a 
            class="CIRCLE" 
            href="/category/"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a 
            class="CIRCLE" 
            href="/tag/"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a 
            class="CIRCLE" 
            href="/link/"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a 
            class="CIRCLE" 
            href="/about/"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a 
            class="CIRCLE" 
            href="/atom.xml"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#为何实现一个BP神经网络？"><span class="toc-text">为何实现一个BP神经网络？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#实现一个BP神经网络的7个步骤"><span class="toc-text">实现一个BP神经网络的7个步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据集"><span class="toc-text">数据集</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#选择人工神经网络结构"><span class="toc-text">选择人工神经网络结构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#随机初始化权重"><span class="toc-text">随机初始化权重</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#网络权重随机初始化"><span class="toc-text">网络权重随机初始化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#实现前向传播"><span class="toc-text">实现前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#前向传播函数定义如下"><span class="toc-text">前向传播函数定义如下</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#实现成本函数"><span class="toc-text">实现成本函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#成本函数的数学表示"><span class="toc-text">成本函数的数学表示</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#实现反向传播"><span class="toc-text">实现反向传播</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度检测"><span class="toc-text">梯度检测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度检测原理"><span class="toc-text">梯度检测原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#这种梯度计算实现如下："><span class="toc-text">这种梯度计算实现如下：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#把一切放到一起"><span class="toc-text">把一切放到一起</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#使用训练集进行训练"><span class="toc-text">使用训练集进行训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#检测两种方式计算的梯度是否近似"><span class="toc-text">检测两种方式计算的梯度是否近似</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#对测试集使用训练得来的参数"><span class="toc-text">对测试集使用训练得来的参数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#另一个例子：手写数字数据集"><span class="toc-text">另一个例子：手写数字数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#载入并观察数据集："><span class="toc-text">载入并观察数据集：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#选择神经网络参数"><span class="toc-text">选择神经网络参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#进行梯度检测"><span class="toc-text">进行梯度检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#对测试集使用训练得来的参数-1"><span class="toc-text">对测试集使用训练得来的参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-text">BY REVERLAND</span></a></li></ol></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="slide-" class="article article-type-slide" itemscope itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">实现一个反向传播人工神经网络</h1>

    <div class="article-meta">
      <span>2014-06-02</span>

      <span> | </span>

      <span class="article-author">Liu Yuyang</span>

      <span> | </span>

      
  <span class="article-category">
    <a class="article-category-link" href="/categories/machine-learning/">machine learning</a>
  </span>


    </div>
  </header>

  <div class="article-content">
    
      <h1 id="为何实现一个BP神经网络？">为何实现一个BP神经网络？</h1><blockquote>
<p>“What I cannot create, I do not understand”</p>
<p>— Richard Feynman,  February 1988</p>
</blockquote>
<hr>
<h2 id="实现一个BP神经网络的7个步骤">实现一个BP神经网络的7个步骤</h2><ol>
<li>选择神经网络 <em>结构</em></li>
<li><em>随机</em> 初始化权重</li>
<li>实现 <em>前向传播</em> </li>
<li>实现 <em>成本函数</em> $J(\Theta)$</li>
<li>实现反向传播算法并计算 <em>偏微分</em> $\frac{\partial}{\partial<br>\Theta_{jk}^{(i)}}J(\Theta)$</li>
<li>使用 <em>梯度检查</em> 并在检查后关闭</li>
<li>使用梯度下降或其它优化算法和反向传播来 <em>优化</em> $\Theta$ 的函数 $J(\Theta)$</li>
</ol>
<hr>
<h2 id="数据集">数据集</h2><p>我们选取著名的鸢尾花数据集作为神经网络作用的对象，首先先观察下数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">2</span>]:</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># random it</span></span><br><span class="line">perm = np.random.permutation(iris.target.size)</span><br><span class="line">iris.data = iris.data[perm]</span><br><span class="line">iris.target = iris.target[perm]</span><br><span class="line"><span class="keyword">print</span> iris.data.shape</span><br><span class="line"><span class="keyword">print</span> iris.target.shape </span><br><span class="line">np.unique(iris.target)</span><br><span class="line"><span class="comment">#print iris</span></span><br><span class="line">Out[<span class="number">2</span>]:</span><br><span class="line">    (<span class="number">150</span>, <span class="number">4</span>)</span><br><span class="line">    (<span class="number">150</span>,)</span><br><span class="line"></span><br><span class="line">    array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>可见，鸢尾花数据集有四个特征，分为三种类别，总共150个条数据。</p>
<hr>
<h1 id="选择人工神经网络结构">选择人工神经网络结构</h1><p><img src="/images/nn.png" alt="神经网络架构图"></p>
<hr>
<h1 id="随机初始化权重">随机初始化权重</h1><h2 id="网络权重随机初始化">网络权重随机初始化</h2><p> 初始化 $\Theta_{ij}^{(l)}$ 为  $[-\epsilon, \epsilon]$ 之间的随机值。</p>
<p> 初始化函数可定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">3</span>]:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init</span><span class="params">(input_layer_size, hidden_layer_size, classes, init_epsilon=<span class="number">0.12</span>)</span>:</span></span><br><span class="line">    Theta_1 = np.random.rand(hidden_layer_size, input_layer_size + <span class="number">1</span>) * <span class="number">2</span> * init_epsilon -init_epsilon</span><br><span class="line">    Theta_2 = np.random.rand(classes, hidden_layer_size + <span class="number">1</span>) * <span class="number">2</span> * init_epsilon -init_epsilon</span><br><span class="line">    <span class="keyword">return</span> (Theta_1, Theta_2)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="实现前向传播">实现前向传播</h1><p>每一层都以此为公式计算下一层：</p>
<p>$$<br>G_{\theta^{(l)}(X)} = \frac{1}{1 + e^{-X(\Theta^{(l)})^T}}<br>$$</p>
<p>注意添加偏差项。</p>
<hr>
<h3 id="前向传播函数定义如下">前向传播函数定义如下</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">4</span>]:</span><br><span class="line"><span class="comment"># 先定义一个sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.e ** (-np.dot(X, np.transpose(theta))))</span><br><span class="line"><span class="comment"># 前向传播函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(Theta_1, Theta_2, X)</span>:</span></span><br><span class="line">    h = g(Theta_1, np.hstack((np.ones((X.shape[<span class="number">0</span>], <span class="number">1</span>)), X)))</span><br><span class="line">    o = g(Theta_2, np.hstack((np.ones((h.shape[<span class="number">0</span>], <span class="number">1</span>)), h)))</span><br><span class="line">    <span class="keyword">return</span> o</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="实现成本函数">实现成本函数</h1><h2 id="成本函数的数学表示">成本函数的数学表示</h2><p>$$<br>J(\Theta) = -\frac{1}{m}[\sum<em>{i=1}^m<br>\sum</em>{k=1}^Ky<em>k^{(i)}\log(h</em>\Theta(x^{(i)}))_k + (1 -<br>y<em>k^{(i)})\log(1-h</em>\Theta(x^{(i)}))<em>k)] + \frac{\lambda}{2m}\sum</em>{l=1}^{L-1}\sum<br>_{i=1}^{s<em>l}\sum</em>{j=1}^{s<em>{l+1}}(\Theta</em>{ji}^{(l)})^2<br>$$</p>
<p>$K$是输出层单元个数, $L$是层数，我们这里是三， $s_l$ 是层 $l$ 中单元数。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">5</span>]:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costfunction</span><span class="params">(nn_param, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span>:</span></span><br><span class="line">    Theta_1 = nn_param[:hidden_layer_size * (input_layer_size + <span class="number">1</span>)].reshape((hidden_layer_size, (input_layer_size + <span class="number">1</span>)))</span><br><span class="line">    Theta_2 = nn_param[hidden_layer_size * (input_layer_size + <span class="number">1</span>):].reshape((classes, (hidden_layer_size + <span class="number">1</span>)))</span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># recode y</span></span><br><span class="line">    y_recoded = np.eye(classes)[y,:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print y_recoded</span></span><br><span class="line">    <span class="comment"># calculate regularator</span></span><br><span class="line">    regularator = (np.sum(Theta_1**<span class="number">2</span>) + np.sum(Theta_2 ** <span class="number">2</span>)) * (lamb_da/(<span class="number">2</span>*m))</span><br><span class="line"></span><br><span class="line">    a3 = predict(Theta_1, Theta_2, X)</span><br><span class="line"></span><br><span class="line">    J = <span class="number">1.0</span> / m * np.sum(-<span class="number">1</span> * y_recoded * np.log(a3)-(<span class="number">1</span>-y_recoded) * np.log(<span class="number">1</span>-a3)) + regularator</span><br><span class="line">    <span class="comment"># print J</span></span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure>
<p>因为scipy中优化函数要求输入参数是向量而不是矩阵，因此我们的函数实现也必须能够随时展开和复原矩阵。</p>
<hr>
<h1 id="实现反向传播">实现反向传播</h1><ul>
<li>正向传播计算网络输出</li>
<li>反向计算每一层的误差$\sigma^{(l)}$</li>
<li>由误差累加计算得到成本函数的偏微分$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$</li>
</ul>
<p>同样，在实现函数时因为要传递给<code>scipy.optimize</code>模块中的优化函数，必须能展开矩阵参数为向量并可随时复原。</p>
<p>详细实现过程参见Andrew NG在Coursera上的讲座。</p>
<p>数学推导过程请 <em>自行</em> 上coursera ml课程论坛搜索。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">6</span>]:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Gradient</span><span class="params">(nn_param, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span>:</span></span><br><span class="line">    </span><br><span class="line">    Theta_1 = nn_param[:hidden_layer_size * (input_layer_size + <span class="number">1</span>)].reshape((hidden_layer_size, (input_layer_size + <span class="number">1</span>)))</span><br><span class="line">    Theta_2 = nn_param[hidden_layer_size * (input_layer_size + <span class="number">1</span>):].reshape((classes, (hidden_layer_size + <span class="number">1</span>)))</span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    Theta1_grad = np.zeros(Theta_1.shape);</span><br><span class="line">    Theta2_grad = np.zeros(Theta_2.shape);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># For the input layer, where l=1:</span></span><br><span class="line">        <span class="comment"># add a bias unit and forward propagate</span></span><br><span class="line">        a1 = np.hstack((np.ones((X[t:t+<span class="number">1</span>,:].shape[<span class="number">0</span>], <span class="number">1</span>)), X[t:t+<span class="number">1</span>,:]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For the hidden layers, where l=2:</span></span><br><span class="line">        a2 = g(Theta_1, a1)</span><br><span class="line">        a2 = np.hstack((np.ones((a2.shape[<span class="number">0</span>], <span class="number">1</span>)), a2))</span><br><span class="line">        a3 = g(Theta_2, a2)</span><br><span class="line"></span><br><span class="line">        yy = np.zeros((<span class="number">1</span>, classes))</span><br><span class="line">        yy[<span class="number">0</span>][y[t]] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># For the delta values:</span></span><br><span class="line">        delta_3 = a3 - yy;</span><br></pre></td></tr></table></figure>
<hr>
<p>续上页</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">    delta_2 = delta_3.dot(Theta_2) * (a2 * (<span class="number">1</span> - a2))</span><br><span class="line">    delta_2 = delta_2[<span class="number">0</span>][<span class="number">1</span>:] <span class="comment"># Taking of the bias row</span></span><br><span class="line">    delta_2 = np.transpose(delta_2)</span><br><span class="line">    delta_2 = delta_2.reshape(<span class="number">1</span>, (np.size(delta_2)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># delta_1 is not calculated because we do not associate error with the input</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Big delta update</span></span><br><span class="line">    Theta1_grad = Theta1_grad + np.transpose(delta_2).dot(a1);</span><br><span class="line">    Theta2_grad = Theta2_grad + np.transpose(delta_3).dot(a2);</span><br><span class="line"></span><br><span class="line">Theta1_grad = <span class="number">1.0</span> / m * Theta1_grad + lamb_da / m * np.hstack((np.zeros((Theta_1.shape[<span class="number">0</span>], <span class="number">1</span>)), Theta_1[:, <span class="number">1</span>:]))</span><br><span class="line">Theta2_grad = <span class="number">1.0</span> / m * Theta2_grad + lamb_da / m * np.hstack((np.zeros((Theta_2.shape[<span class="number">0</span>], <span class="number">1</span>)), Theta_2[:, <span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">Grad = np.concatenate((Theta1_grad.ravel(), Theta2_grad.ravel()), axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> Grad</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="梯度检测">梯度检测</h1><blockquote>
<p>“But back prop as an algorithm has a lot of details and,you know, can be a<br>little bit tricky to implement.”</p>
<p>– Andrew NG如是说</p>
</blockquote>
<ul>
<li>即使看上去成本函数在一直减小，可能神经网络还是有问题。</li>
<li>梯度检测能让你确认，哦，我的实现还正常。</li>
</ul>
<hr>
<h2 id="梯度检测原理">梯度检测原理</h2><p>简单的微分近似。但相比反向传播神经网络算法计算量更大。</p>
<p>$$<br>\frac{\partial}{\partial\theta<em>{i}}J(\theta) \approx<br>\frac{J(\theta</em>{i}+\epsilon) - J(\theta_{i}+\epsilon) }{2\epsilon}<br>$$</p>
<h3 id="这种梯度计算实现如下：">这种梯度计算实现如下：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">7</span>]:</span><br><span class="line"><span class="comment"># define compute_grad</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_grad</span><span class="params">(theta, input_layer_size, hidden_layer_size, classes, X, y, lamb_da, epsilon=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">    n = np.size(theta)</span><br><span class="line">    gradaproxy = np.zeros(n)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        theta_plus = np.copy(theta) <span class="comment"># Important for in numpy assign is shalow copy</span></span><br><span class="line">        theta_plus[i] = theta_plus[i] + epsilon</span><br><span class="line">        theta_minus = np.copy(theta)</span><br><span class="line">        theta_minus[i] = theta_minus[i] - epsilon</span><br><span class="line">        gradaproxy[i] = (costfunction(theta_plus, input_layer_size, hidden_layer_size, classes, X, y, lamb_da) - costfunction(theta_minus, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)) / (<span class="number">2.0</span> * epsilon)</span><br><span class="line">    <span class="keyword">return</span> gradaproxy</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="把一切放到一起">把一切放到一起</h1><p>先定义训练函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">8</span>]:</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> optimize</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span>:</span></span><br><span class="line">    Theta_1, Theta_2 = random_init(input_layer_size, hidden_layer_size, classes, init_epsilon=<span class="number">0.12</span>)</span><br><span class="line">    nn_param = np.concatenate((Theta_1.ravel(), Theta_2.ravel()))</span><br><span class="line">    final_nn = optimize.fmin_cg(costfunction, </span><br><span class="line">                                np.concatenate((Theta_1.ravel(), Theta_2.ravel()), axis=<span class="number">0</span>), </span><br><span class="line">                                fprime=Gradient, </span><br><span class="line">                                args=(input_layer_size,</span><br><span class="line">                                      hidden_layer_size, </span><br><span class="line">                                      classes, </span><br><span class="line">                                      X, </span><br><span class="line">                                      y,</span><br><span class="line">                                      lamb_da))</span><br><span class="line">    <span class="keyword">return</span> final_nn</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="使用训练集进行训练">使用训练集进行训练</h2><p>我选取鸢尾花数据集的100条作为训练集，剩下50条作为测试集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">9</span>]:</span><br><span class="line">X = iris.data[:<span class="number">100</span>]</span><br><span class="line">y = iris.target[:<span class="number">100</span>]</span><br><span class="line">lamb_da = <span class="number">1.0</span> <span class="comment"># must be float</span></span><br><span class="line">input_layer_size = <span class="number">4</span></span><br><span class="line">hidden_layer_size = <span class="number">6</span></span><br><span class="line">classes = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">final_nn = train(input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span><br><span class="line"></span><br><span class="line">    Warning: Desired error <span class="keyword">not</span> necessarily achieved due to precision loss.</span><br><span class="line">             Current function value: <span class="number">0.878999</span></span><br><span class="line">             Iterations: <span class="number">48</span></span><br><span class="line">             Function evaluations: <span class="number">152</span></span><br><span class="line">             Gradient evaluations: <span class="number">140</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="检测两种方式计算的梯度是否近似">检测两种方式计算的梯度是否近似</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">10</span>]:</span><br><span class="line"><span class="comment"># gradient checking</span></span><br><span class="line">grad_aprox = compute_grad(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span><br><span class="line">grad_bp = Gradient(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span><br><span class="line"><span class="keyword">print</span> (grad_aprox - grad_bp) &lt; <span class="number">1e-1</span></span><br><span class="line"></span><br><span class="line">    [ <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span></span><br><span class="line">      <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span></span><br><span class="line">      <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span></span><br><span class="line">      <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span></span><br><span class="line">      <span class="keyword">True</span>  <span class="keyword">True</span>  <span class="keyword">True</span>]</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="对测试集使用训练得来的参数">对测试集使用训练得来的参数</h2><p>可以看到，测试集中50个样本有_个判定错误，其它_个分类正确。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">11</span>]:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span>:</span></span><br><span class="line">    Theta_1 = final_nn[:hidden_layer_size * (input_layer_size + <span class="number">1</span>)].reshape((hidden_layer_size, input_layer_size + <span class="number">1</span>))</span><br><span class="line">    Theta_2 = final_nn[hidden_layer_size * (input_layer_size + <span class="number">1</span>):].reshape((classes, hidden_layer_size + <span class="number">1</span>))</span><br><span class="line">    nsuccess = np.sum(np.argmax(predict(Theta_1, Theta_2, X), axis=<span class="number">1</span>) == y)</span><br><span class="line">    <span class="keyword">return</span> nsuccess</span><br><span class="line"></span><br><span class="line">n = test(final_nn, input_layer_size, hidden_layer_size, classes, iris.data[<span class="number">100</span>:], iris.target[<span class="number">100</span>:], lamb_da)</span><br><span class="line"><span class="keyword">print</span> n</span><br><span class="line">n = test(final_nn, input_layer_size, hidden_layer_size, classes, iris.data[:<span class="number">100</span>], iris.target[:<span class="number">100</span>], lamb_da)</span><br><span class="line"><span class="keyword">print</span> n</span><br><span class="line"></span><br><span class="line">Out[<span class="number">11</span>]:</span><br><span class="line">    <span class="number">47</span></span><br><span class="line">    <span class="number">99</span></span><br></pre></td></tr></table></figure>
<hr>
<h1 id="另一个例子：手写数字数据集">另一个例子：手写数字数据集</h1><p>最后是对同样著名的手写数字数据集的实验。</p>
<h3 id="载入并观察数据集：">载入并观察数据集：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">12</span>]:</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line"></span><br><span class="line"><span class="comment"># random it</span></span><br><span class="line">perm = np.random.permutation(digits.target.size)</span><br><span class="line">digits.data = digits.data[perm]</span><br><span class="line">digits.target = digits.target[perm]</span><br><span class="line"><span class="keyword">print</span> digits.data.shape</span><br><span class="line"><span class="keyword">print</span> digits.target.shape</span><br><span class="line"><span class="keyword">print</span> np.unique(digits.target)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">12</span>]:</span><br><span class="line">    (<span class="number">1797</span>, <span class="number">64</span>)</span><br><span class="line">    (<span class="number">1797</span>,)</span><br><span class="line">    [<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="选择神经网络参数">选择神经网络参数</h2><p>取1000个样本作为训练集，剩下作为测试集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">13</span>]:</span><br><span class="line">X = digits.data[:<span class="number">1000</span>]</span><br><span class="line">y = digits.target[:<span class="number">1000</span>]</span><br><span class="line">lamb_da = <span class="number">1.0</span> <span class="comment"># must be float</span></span><br><span class="line">input_layer_size = <span class="number">64</span></span><br><span class="line">hidden_layer_size = <span class="number">10</span></span><br><span class="line">classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">final_nn = train(input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">13</span>]:</span><br><span class="line">    Warning: Desired error <span class="keyword">not</span> necessarily achieved due to precision loss.</span><br><span class="line">             Current function value: <span class="number">0.594474</span></span><br><span class="line">             Iterations: <span class="number">965</span></span><br><span class="line">             Function evaluations: <span class="number">2210</span></span><br><span class="line">             Gradient evaluations: <span class="number">2189</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="进行梯度检测">进行梯度检测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">14</span>]:</span><br><span class="line"><span class="comment"># gradient checking</span></span><br><span class="line">grad_aprox = compute_grad(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span><br><span class="line">grad_bp = Gradient(final_nn, input_layer_size, hidden_layer_size, classes, X, y, lamb_da)</span><br><span class="line"><span class="keyword">print</span> np.all((grad_aprox - grad_bp) &lt; <span class="number">1e-1</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">14</span>]:</span><br><span class="line">    <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="对测试集使用训练得来的参数-1">对测试集使用训练得来的参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In[<span class="number">15</span>]:</span><br><span class="line"></span><br><span class="line">n = test(final_nn, input_layer_size, hidden_layer_size, classes, digits.data[<span class="number">1000</span>:], digits.target[<span class="number">1000</span>:], lamb_da)</span><br><span class="line"><span class="keyword">print</span> n</span><br><span class="line"></span><br><span class="line">n = test(final_nn, input_layer_size, hidden_layer_size, classes, digits.data[:<span class="number">1000</span>], digits.target[:<span class="number">1000</span>], lamb_da)</span><br><span class="line"><span class="keyword">print</span> n</span><br><span class="line"></span><br><span class="line">Out[<span class="number">15</span>]:</span><br><span class="line"></span><br><span class="line">    <span class="number">722</span></span><br><span class="line">    <span class="number">991</span></span><br></pre></td></tr></table></figure>
<hr>
<center><br><h3>BY REVERLAND</h3><br></center>



    
  </div>
</article>

</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal-one" aria-hidden="true">
  <a href="#close" class="cover" aria-hidden="true"></a>
  <div class="modal-dialog">
    <div class="modal-header">
      <a href="#close" class="btn-close" aria-hidden="true">关闭</a>
    </div>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-slide">
    
  <section class="disqus-comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>
  </section>

  <script>
    var disqus_shortname = 'reverlandblog';
    
    var disqus_url = 'http://reverland.org/machine-learning/2014/06/02//';
    
    (function(){
      var dsq = document.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>


    




  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/fastclick.js', function() {
      loadScript('/js/app.js', function() {
        // load success
      });
    });
  }
</script>

</body>
</html>
